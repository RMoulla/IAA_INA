{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/IAA_INA/blob/main/TP_RAG_Github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq4_SiElyWSK"
      },
      "source": [
        "# **TP : Retrieval Augmented Generation (RAG)**\n",
        "\n",
        "Dans ce TP, nous allons construire un **Retrieval Augmented Generation (RAG)**. L'objectif est de fournir une méthode complète pour traiter un fichier PDF, en extraire des informations pertinentes, puis les utiliser pour répondre à une requête utilisateur de manière augmentée. Ce TP est divisé en trois phases distinctes :\n",
        "\n",
        "1. **Extraction et segmentation du texte** : Vous allez apprendre à extraire du texte à partir d'un fichier PDF et à le segmenter en sections de taille définie.\n",
        "2. **Calcul des embeddings et recherche des segments similaires** : Nous utiliserons des techniques d'embeddings pour trouver les sections du texte les plus pertinentes par rapport à une requête donnée.\n",
        "3. **Génération de réponses avec un modèle de type ChatGPT** : En utilisant le contexte extrait et le modèle GPT-4, nous allons générer une réponse contextualisée à la question posée par l'utilisateur.\n",
        "\n",
        "Par ailleurs, nous allons utiliser des outils comme `pdfplumber`, pour parser les documents pdf, `sentence-transformers` pour les embeddings et l'API OpenAI pour générer des réponses augmentées par le contexte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCdntTJgyWSN"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQmQodTmyWSO"
      },
      "source": [
        "## **Étape 1 : Extraction et segmentation du texte**\n",
        "\n",
        "Dans cette première étape, nous allons nous concentrer sur l'extraction du texte d'un fichier PDF et sa segmentation en blocs de taille définie. L'objectif est de rendre le texte extrait plus facile à traiter dans les phases suivantes.\n",
        "\n",
        "### Détails :\n",
        "1. **Extraction du texte** : À l'aide de `pdfplumber`, vous allez extraire tout le texte d'un fichier PDF donné. Cette étape vous permettra d'obtenir une version brute du contenu du document, qui servira de base pour les autres étapes.\n",
        "   \n",
        "2. **Segmentation du texte** : Une fois le texte extrait, vous le diviserez en segments de taille fixe (par exemple, 500 caractères par segment) à l'aide de la fonction `textwrap.wrap()` de Python. Cela permet de découper le texte en morceaux plus faciles à manipuler dans la phase suivante où nous calculerons les similarités.\n",
        "\n",
        "Cette étape est cruciale, car elle permet de structurer le texte pour le rendre exploitable par la suite. La qualité de l'extraction et de la segmentation influencera directement les résultats des phases suivantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxl0bh8gyWSP"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "import textwrap\n",
        "\n",
        "# Chemin vers le fichier PDF\n",
        "pdf_path = \"IAY.pdf\"\n",
        "\n",
        "# Extraction du texte et des tables\n",
        "full_text = \"\"\n",
        "tables = []  # Liste pour stocker les tables extraites\n",
        "\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    for page in pdf.pages:\n",
        "        # Extraction du texte\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            full_text += page_text\n",
        "\n",
        "        # Extraction des tables\n",
        "        page_tables = page.extract_tables()\n",
        "        if page_tables:\n",
        "            tables.extend(page_tables)  # Ajouter les tables extraites à la liste\n",
        "\n",
        "# Segmenter le texte en utilisant textwrap.wrap avec une longueur maximale de caractères\n",
        "\n",
        "paragraphs = textwrap.wrap(full_text.strip(), width=500)\n",
        "\n",
        "# Affichage des segments extraits\n",
        "print(f\"Nombre de segments de texte extraits : {len(paragraphs)}\")\n",
        "for i, para in enumerate(paragraphs[:5], 1):\n",
        "    print(f\"Segment {i}: {para}\\n\")\n",
        "\n",
        "# Affichage des tables extraites\n",
        "if tables:\n",
        "    print(f\"Nombre de tables extraites : {len(tables)}\")\n",
        "    print(\"Première table extraite :\")\n",
        "    for row in tables[0]:\n",
        "        print(row)\n",
        "else:\n",
        "    print(\"Aucune table trouvée dans le PDF.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhKKNDsWyWSP"
      },
      "source": [
        "## **Étape 2 : Calcul des Embeddings et recherche des segments similaires**\n",
        "\n",
        "Dans cette deuxième étape, nous allons calculer des **embeddings** pour chaque segment de texte extrait et effectuer une recherche pour identifier les segments les plus similaires à une requête utilisateur donnée.\n",
        "\n",
        "### Détails :\n",
        "1. **Calcul des embeddings** : Nous allons utiliser la bibliothèque `sentence-transformers` pour générer des vecteurs d’embeddings pour chaque segment de texte. Ces vecteurs capturent les caractéristiques sémantiques des segments, permettant ainsi de comparer leur pertinence par rapport à une requête donnée.\n",
        "\n",
        "2. **Recherche des segments les plus similaires** : Une fois les embeddings calculés, nous allons utiliser la **similarité cosinus** pour mesurer la proximité entre l’embedding de la requête utilisateur et les embeddings des segments de texte. Les segments les plus proches seront sélectionnés pour la prochaine étape.\n",
        "\n",
        "3. **Sélection des top-N segments** : Nous sélectionnerons les `n` segments les plus similaires à la requête, qui serviront de contexte pour générer une réponse augmentée dans la phase suivante.\n",
        "\n",
        "Cette étape est essentielle pour filtrer le contenu extrait en fonction de sa pertinence par rapport à la requête utilisateur. Elle permet de s'assurer que seuls les segments les plus utiles sont utilisés pour générer une réponse cohérente et contextuelle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxYFpF3WyWSQ"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF8DeQznyWSQ"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Charger un modèle pré-entraîné de sentence-transformers\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ldt1IB_WyWSQ"
      },
      "outputs": [],
      "source": [
        "# Calcul des embeddings pour chaque segmnt de texte (tableaux NumPy par défaut)\n",
        "embeddings = model.encode(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPtbv8EwyWSQ"
      },
      "outputs": [],
      "source": [
        "len(embeddings[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OROr4irsyWSR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Requête utilisateur pour laquelle nous cherchons les segments similaires\n",
        "query = \"Quels sont les LLMs open source ?\"\n",
        "query_embedding = model.encode(query)\n",
        "\n",
        "# Calcul des similarités cosinus entre la requête et les segments\n",
        "similarities = util.cos_sim(query_embedding, embeddings)\n",
        "\n",
        "# Sélection des top-N documents les plus similaires\n",
        "top_n = 10\n",
        "top_results = similarities.topk(top_n)[1]  # Indices des top-N documents similaires\n",
        "\n",
        "# Accéder aux indices des résultats top-N\n",
        "top_n_paragraphs = [paragraphs[i] for i in top_results[0]]\n",
        "\n",
        "# Affichage des top-N segments les plus similaires\n",
        "for i, para in enumerate(top_n_paragraphs, 1):\n",
        "    print(f\"Segment {i} similaire : {para}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPhd5daJyWSR"
      },
      "outputs": [],
      "source": [
        "len(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdXJ1aUpyWSR"
      },
      "source": [
        "## **Étape 3 : Génération de réponses avec ChatGPT**\n",
        "\n",
        "Dans cette troisième, nous allons utiliser les segments de texte sélectionnés et une requête utilisateur pour générer une réponse contextuelle en utilisant l'API OpenAI, avec un modèle de type **ChatGPT**.\n",
        "\n",
        "### Objectifs :\n",
        "- Utiliser le modèle **GPT-4** pour générer une réponse basée sur les segments de texte les plus similaires à la requête.\n",
        "- Envoyer les segments sélectionnés et la requête utilisateur sous forme de **messages** à l’API ChatGPT.\n",
        "- Obtenir une réponse contextuelle, augmentée par les informations pertinentes extraites du texte.\n",
        "\n",
        "### Détails :\n",
        "1. **Création du contexte** : Nous allons concaténer les segments sélectionnés lors de l’étape précédente afin de créer un contexte cohérent à transmettre au modèle GPT-4. Ce contexte servira à fournir un maximum d’informations pertinentes pour générer une réponse précise.\n",
        "\n",
        "2. **Appel à l’API ChatGPT** : En utilisant le contexte et la requête de l’utilisateur, nous formulerons un prompt que nous enverrons à l’API ChatGPT. Le prompt sera structuré sous forme de messages (avec les rôles \"system\" et \"user\") pour que le modèle comprenne le contexte de la conversation.\n",
        "\n",
        "3. **Génération et récupération de la réponse** : Le modèle GPT-4 générera une réponse basée sur le contexte fourni. La réponse sera ensuite récupérée et affichée comme résultat final.\n",
        "\n",
        "Cette étape finalise le processus de **RAG** en combinant la recherche d’informations pertinentes dans un corpus de texte et la génération de réponses intelligentes basées sur ces informations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYNJe3EJyWSS"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTcmt9HvyWSS"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "\n",
        "openai.api_key = \"\"\n",
        "\n",
        "# Concaténer les top-N segments en un seul contexte\n",
        "context = \"\\n\".join(top_n_paragraphs)\n",
        "prompt = f\"Contexte :\\n{context}\\n\\nQuestion : {query}\\nRéponse :\"\n",
        "\n",
        "# Appeler l'API d'OpenAI\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4o\",  # Utilisation du modèle gpt-4o\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Tu es un assistant specialisé dans la recherche d'information à partir de documents fournis. Tes réponses doivent absolument provenir du contexte fourni.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Extraire et afficher la réponse générée\n",
        "generated_response = response['choices'][0]['message']['content'].strip()\n",
        "print(f\"Réponse générée : {generated_response}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}